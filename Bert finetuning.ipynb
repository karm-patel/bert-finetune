{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e02b46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karm/miniconda3/envs/torch310/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torchmetrics import Accuracy, F1Score\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "RAW_DATASET_DIR = \"raw_datasets/SST_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06ded53b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cpu_device = torch.device(\"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd07e634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room floor of any given daytime soap</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love , memory , history and the war between art and commerce</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been the be all end all of the modern office anomie films</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6915</th>\n",
       "      <td>painful , horrifying and oppressively tragic , this film should not be missed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6916</th>\n",
       "      <td>take care is nicely performed by a quintet of actresses , but nonetheless it drags during its 112 minute length</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6917</th>\n",
       "      <td>the script covers huge , heavy topics in a bland , surfacey way that does n't offer any insight into why , for instance , good things happen to bad people</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6918</th>\n",
       "      <td>a seriously bad film with seriously warped logic by writer director kurt wimmer at the screenplay level</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6919</th>\n",
       "      <td>a deliciously nonsensical comedy about a city coming apart at its seams</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6920 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                    0  \\\n",
       "0                                                                                                                             a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films   \n",
       "1                                                                                                                                                        apparently reassembled from the cutting room floor of any given daytime soap   \n",
       "2     they presume their audience wo n't sit still for a sociology lesson , however entertainingly presented , so they trot out the conventional science fiction elements of bug eyed monsters and futuristic women in skimpy clothes   \n",
       "3                                                                                                                              this is a visually stunning rumination on love , memory , history and the war between art and commerce   \n",
       "4                                                                                                                                   jonathan parker 's bartleby should have been the be all end all of the modern office anomie films   \n",
       "...                                                                                                                                                                                                                               ...   \n",
       "6915                                                                                                                                                    painful , horrifying and oppressively tragic , this film should not be missed   \n",
       "6916                                                                                                                  take care is nicely performed by a quintet of actresses , but nonetheless it drags during its 112 minute length   \n",
       "6917                                                                       the script covers huge , heavy topics in a bland , surfacey way that does n't offer any insight into why , for instance , good things happen to bad people   \n",
       "6918                                                                                                                          a seriously bad film with seriously warped logic by writer director kurt wimmer at the screenplay level   \n",
       "6919                                                                                                                                                          a deliciously nonsensical comedy about a city coming apart at its seams   \n",
       "\n",
       "      1  \n",
       "0     1  \n",
       "1     0  \n",
       "2     0  \n",
       "3     1  \n",
       "4     1  \n",
       "...  ..  \n",
       "6915  1  \n",
       "6916  0  \n",
       "6917  0  \n",
       "6918  0  \n",
       "6919  1  \n",
       "\n",
       "[6920 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\\t', header=None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0e99a",
   "metadata": {},
   "source": [
    "### Word piece tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e14f3870",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model_name = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cc1be70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "sequence = \"A Titan RTX has 24GB of VRAM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9017fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'titan', 'rt', '##x', 'has', '24', '##gb', 'of', 'vr', '##am']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "print(tokenized_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3b227c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1037, 16537, 19387, 2595, 2038, 2484, 18259, 1997, 27830, 3286, 102]\n"
     ]
    }
   ],
   "source": [
    "encoded_sequence = tokenizer(sequence)[\"input_ids\"]\n",
    "print(encoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0353c3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] a titan rtx has 24gb of vram [SEP]\n"
     ]
    }
   ],
   "source": [
    "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
    "print(decoded_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240c009c",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec2d0cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertDataset(Dataset):\n",
    "    def __init__(self, tokenizer,max_length, device=torch.device(\"cuda\"), split=\"train\"):\n",
    "        super(BertDataset, self).__init__()\n",
    "        self.data_df=pd.read_csv(os.path.join(RAW_DATASET_DIR, split + \".tsv\"), delimiter='\\t')\n",
    "        self.tokenizer=tokenizer\n",
    "        self.target=self.data_df.iloc[:,1]\n",
    "        self.max_length=max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text1 = self.data_df.iloc[index,0].lower()\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text1 ,\n",
    "            None, # since we have only 1 sentence as input\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long).to(device),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long).to(device),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(device),\n",
    "            'target': torch.tensor(self.data_df.iloc[index, 1], dtype=torch.long).to(device)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbd0f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model_name)\n",
    "BATCH_SIZE = 32\n",
    "MAX_SENT_LENGTH = 56\n",
    "\n",
    "# train dataset\n",
    "train_dataset= BertDataset(tokenizer, max_length=MAX_SENT_LENGTH, split=\"train\")\n",
    "train_dataloader=DataLoader(dataset=train_dataset,batch_size=BATCH_SIZE)\n",
    "\n",
    "# dev dataset\n",
    "dev_dataset= BertDataset(tokenizer, max_length=MAX_SENT_LENGTH, split=\"dev\")\n",
    "dev_dataloader=DataLoader(dataset=dev_dataset,batch_size=BATCH_SIZE)\n",
    "\n",
    "# test dataset\n",
    "test_dataset= BertDataset(tokenizer, max_length=MAX_SENT_LENGTH, split=\"test\")\n",
    "test_dataloader=DataLoader(dataset=test_dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "247280cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/karm/miniconda3/envs/torch310/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# testing data loaders\n",
    "next(iter(train_dataloader));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e52968a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec5e802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, d_model=768, H = 50, n_classes=2):\n",
    "        super(BERT, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained(bert_model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, n_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self,ids,mask,token_type_ids):\n",
    "        # need to pass positional embeddings\n",
    "        last_hidden_states, _ = self.bert_model(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        # last_hidden_state.shape = (batch_size, sequence_length, hidden_size)\n",
    "        \n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
    "        cls = last_hidden_states[:, 0, :] # (Batch, MAX_length, hidden_size)\n",
    "        logits = self.classifier(cls)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9283ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77cdefd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Keyword arguments {'apad_to_max_length': True} not recognized.\n",
      "Keyword arguments {'apad_to_max_length': True} not recognized.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 1045, 1005, 1049, 6251, 1015, 102, 1045, 2572, 2183, 2000, 2310, 1037, 3416, 6251, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained(bert_model_name)\n",
    "op = tokenizer.encode_plus(\n",
    "            \"I'm sentence 1\", # text\n",
    "            \"I am going to ve a 2nd sentence\", # text pair\n",
    "            apad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=56,\n",
    "        )\n",
    "op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fddc30f",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e414841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BERT(d_model=768, H=50).to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "#Initialize Optimizer\n",
    "lr = 5e-5\n",
    "optimizer= optim.Adam(model.parameters(),lr= lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e0360132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only finetune classification head\n",
    "# for param in model.bert_model.parameters():\n",
    "#     param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "014b065e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    loop=tqdm(enumerate(train_dataloader),leave=False,total=len(train_dataloader))\n",
    "    print(epoch)\n",
    "    total_matches = 0\n",
    "    for batch, dl in loop:\n",
    "        \n",
    "        # input\n",
    "        ids, token_type_ids, mask, label = dl['ids'], dl['token_type_ids'], dl['mask'], dl['target']\n",
    "        optimizer.zero_grad()\n",
    "        output=F.softmax(model(ids=ids,mask=mask,token_type_ids=token_type_ids), dim=1) # (B, 2)\n",
    "        \n",
    "        # loss\n",
    "        loss=loss_fn(output,label)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # prediction\n",
    "        pred = torch.argmax(output, dim=-1) # (B,1)\n",
    "        accuracy = Accuracy(task = \"binary\").to(device)(pred, label)\n",
    "        total_matches += (torch.sum(pred == label)).item()\n",
    "\n",
    "        # Show progress while training\n",
    "        loop.set_description_str(f\"Epoch={epoch}/{epochs} loss={loss.item()} acc={accuracy}\")\n",
    "\n",
    "    \n",
    "    print(f\"Train Accuracy :{epoch} = {total_matches/len(train_dataset)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "55dde614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model_name = f\"{model_name}_SST2_{epochs}_{lr}_FULL.pt\"\n",
    "torch.save(model.state_dict(), f\"ckpts/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f860a6b8",
   "metadata": {},
   "source": [
    "## Tesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f419e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataset\n",
    "test_losses = []\n",
    "\n",
    "model.eval()\n",
    "loop=tqdm(enumerate(test_dataloader),leave=False,total=len(test_dataloader))\n",
    "total_matches = 0\n",
    "with torch.no_grad():\n",
    "    for batch, dl in loop:\n",
    "        ids, token_type_ids, mask, label = dl['ids'], dl['token_type_ids'], dl['mask'], dl['target']\n",
    "        output=F.softmax(model(ids=ids,mask=mask,token_type_ids=token_type_ids), dim=1)\n",
    "        loss=loss_fn(output,label)\n",
    "        test_losses.append(loss)\n",
    "\n",
    "        pred = torch.argmax(output, dim=-1)\n",
    "\n",
    "        matches = torch.sum(pred == label)\n",
    "        total_matches += matches.item()\n",
    "\n",
    "        # Show progress while training\n",
    "        loop.set_description(f'loss={loss.item()}')\n",
    "\n",
    "\n",
    "    print(f\"Test Accuracy :{total_matches/len(test_dataset)}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a3fcee",
   "metadata": {},
   "source": [
    "## Custom Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80755417",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertCustomDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length, sentences, device=torch.device(\"cuda\")):\n",
    "        super(BertCustomDataset, self).__init__()\n",
    "        self.data_df=pd.DataFrame(custom_sentences, columns=[\"text\"])\n",
    "        self.tokenizer=tokenizer\n",
    "        self.max_length=max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        text1 = self.data_df.iloc[index,0]\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text1 ,\n",
    "            None,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long).to(device),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long).to(device),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).to(device),\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "c1f37bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/karm/miniconda3/envs/torch310/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You made 100 run in only 50 balls</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.999990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>You made only 1 run in 50 balls</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wow! You made only 1 run in 50 balls</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.667920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Despite being a topper you are just passed</td>\n",
       "      <td>Negative</td>\n",
       "      <td>0.581516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Despite being a back bencher you are passed</td>\n",
       "      <td>Positive</td>\n",
       "      <td>0.999947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         input Prediction  Probability\n",
       "0            You made 100 run in only 50 balls   Negative     0.999990\n",
       "1              You made only 1 run in 50 balls   Negative     0.999993\n",
       "2         Wow! You made only 1 run in 50 balls   Negative     0.667920\n",
       "3   Despite being a topper you are just passed   Negative     0.581516\n",
       "4  Despite being a back bencher you are passed   Positive     0.999947"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_sentences = [\"I do not like food there.\", \"I like these mangoes\", \"You made 1 run in only 50 balls\", \"Opps! You made 1 run in only 50 balls\", \"Wow! You made 1 run in only 50 balls\", \"You made only 1 run in 50 balls\", \"Opps! You made only 1 run in 50 balls\", \"Wow! You made only 1 run in 50 balls\"]\n",
    "custom_sentences = [\"You made 100 run in only 50 balls\", \"You made only 1 run in 50 balls\", \"Wow! You made only 1 run in 50 balls\", \"Despite being a topper you are just passed\", \"Despite being a back bencher you are passed\"]\n",
    "\n",
    "custom_dataset = BertCustomDataset(tokenizer, 56, custom_sentences)\n",
    "custom_dataloader = DataLoader(custom_dataset, batch_size=len(custom_sentences))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    data = next(iter(custom_dataloader))\n",
    "    ids, token_type_ids, mask = data['ids'], data['token_type_ids'], data['mask']\n",
    "    output=F.softmax(model(ids=ids,mask=mask,token_type_ids=token_type_ids), dim=1)\n",
    "#     print(\"Prediction:\")\n",
    "#     print(list(zip(custom_sentences, torch.argmax(output, dim=1))))\n",
    "    \n",
    "df = pd.DataFrame(custom_sentences, columns=[\"input\"])\n",
    "df[\"Prediction\"] = torch.argmax(output, dim=-1).to(cpu_device)\n",
    "df[\"Prediction\"] = df[\"Prediction\"].apply(lambda x: {0: \"Negative\", 1: \"Positive\"}[x])\n",
    "df[\"Probability\"] = torch.max(output, dim=-1).values.to(cpu_device)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f873f085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I do not like food there.</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I like these mangoes</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Opps! You made 1 runs in only 50 balls</td>\n",
       "      <td>0</td>\n",
       "      <td>0.999992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow! You made only 1 run in 50 balls</td>\n",
       "      <td>0</td>\n",
       "      <td>0.667919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     text  Prediction  Probability\n",
       "0               I do not like food there.           0     0.999993\n",
       "1                    I like these mangoes           1     0.999988\n",
       "2  Opps! You made 1 runs in only 50 balls           0     0.999992\n",
       "3    Wow! You made only 1 run in 50 balls           0     0.667919"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch310]",
   "language": "python",
   "name": "conda-env-torch310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
